{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusterPy: Library of spatially constrained clustering algorithms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python2.7/site-packages/pysal/__init__.py:65: VisibleDeprecationWarning: PySAL's API will be changed on 2018-12-31. The last release made with this API is version 1.14.4. A preview of the next API version is provided in the `pysal` 2.0 prelease candidate. The API changes and a guide on how to change imports is provided at https://pysal.org/about\n",
      "  ), VisibleDeprecationWarning)\n",
      "/anaconda3/envs/ml/lib/python2.7/site-packages/numba/decorators.py:33: NumbaDeprecationWarning: \u001b[1mautojit is deprecated, use jit instead, which provides the same functionality. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numba-autojit\u001b[0m\n",
      "  warnings.warn(NumbaDeprecationWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "from osgeo import ogr, gdal, osr\n",
    "import fiona\n",
    "\n",
    "from shapely.geometry import shape, Point, Polygon, MultiLineString, MultiPoint, MultiPolygon, LineString\n",
    "from shapely import affinity\n",
    "\n",
    "from scipy.ndimage import morphology\n",
    "\n",
    "from utilities import *\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "\n",
    "from osgeo import ogr, gdal, osr\n",
    "import fiona\n",
    "from shapely.geometry import shape, Point, Polygon, MultiLineString, MultiPoint, MultiPolygon, LineString\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "inline_rc = dict(mpl.rcParams)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import clusterpy\n",
    "\n",
    "\n",
    "import itertools\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "\n",
    "import pysal as ps\n",
    "from sklearn import cluster\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "# %pylab inline\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram, set_link_color_palette\n",
    "# from fastcluster import linkage\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import rgb2hex, colorConverter\n",
    "# from matplotlib.pyplot import *\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_filepath = '_input/network.shp'\n",
    "island_filepath = '_input/islands.shp'\n",
    "patch_filepath = '_input/patches.shp' # outlines of channels, not shorelines\n",
    "\n",
    "calculate_params = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load island shapefile\n",
    "islands, _ = load_shapefile(island_filepath)\n",
    "\n",
    "# load patch shapefile\n",
    "patches, _ = load_shapefile(patch_filepath)\n",
    "\n",
    "# load network shapefile\n",
    "network_lines, params = load_shapefile(network_filepath, parameters = ['Width'])\n",
    "network_widths = params['Width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify network lines that surround islands (bounding channels) or that drain islands (interior channels)\n",
    "\n",
    "if calculate_params:\n",
    "\n",
    "    # get midpoints of all network lines, with buffers so they touch both neighbors\n",
    "    midpts = [l.interpolate(0.5, normalized=True).buffer(5) for l in network_lines]\n",
    "\n",
    "    bounding_channels = []\n",
    "    interior_channels = []\n",
    "\n",
    "    # compare the location of midpts to island patches\n",
    "    for polygon in patches:\n",
    "\n",
    "        # if midpoint intersects patch, the line is a bounding channel\n",
    "        touch = [i for i,l in enumerate(midpts) if polygon.exterior.intersects(l)]\n",
    "        bounding_channels.append(touch)\n",
    "\n",
    "        # if midpoint is completely within patch, the line is an interior channel\n",
    "        touch = [i for i,l in enumerate(midpts) if polygon.contains(l)]\n",
    "        interior_channels.append(touch)\n",
    "\n",
    "    pickle.dump(bounding_channels, open( '_input_processed/bounding_channels' + '.p', \"wb\" ) )\n",
    "    pickle.dump(interior_channels, open( '_input_processed/interior_channels' + '.p', \"wb\" ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    bounding_channels = pickle.load( open( '_input_processed/bounding_channels.p', \"rb\" ))\n",
    "    interior_channels = pickle.load( open( '_input_processed/interior_channels.p', \"rb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base metrics\n",
    "\n",
    "interior_lengths = [sum([network_lines[j].length for j in interior_channels[i]]) if len(interior_channels[i])>0 else 0 for i in range(len(islands))] \n",
    "\n",
    "perimeter = np.array([i.boundary.length for i in islands])\n",
    "wetted_perimeter = perimeter + 2 * np.array(interior_lengths)   \n",
    "area = np.array([i.area for i in islands])\n",
    "perimeter_convex_hull = np.array([i.convex_hull.exterior.length for i in islands])\n",
    "area_convex_hull = np.array([i.convex_hull.area for i in islands])\n",
    "\n",
    "a = np.array(map(Polygon_axes, islands))\n",
    "major_axis = a[:,1]\n",
    "minor_axis = a[:,0]\n",
    "aspect_ratio = major_axis / minor_axis\n",
    "\n",
    "circularity = 4 * np.pi * area / perimeter**2\n",
    "equivalent_area_diameter = np.sqrt((4 / np.pi) * area)\n",
    "perimeter_equivalent_diameter = area / np.pi\n",
    "solidity = area / area_convex_hull\n",
    "concavity = area_convex_hull - area\n",
    "convexity = perimeter_convex_hull / perimeter\n",
    "dry_shape_factor = perimeter / np.sqrt(area)\n",
    "wet_shape_factor = wetted_perimeter / np.sqrt(area)\n",
    "\n",
    "polygon_metrics = {'Area': area,\n",
    "                'Perimeter': perimeter,\n",
    "                'WetPerim': wetted_perimeter,\n",
    "                'CH_Area': area_convex_hull,\n",
    "                'CH_Perim': perimeter_convex_hull,\n",
    "                'AspectR': aspect_ratio,\n",
    "                'Circular': circularity,\n",
    "                'Solidity': solidity,\n",
    "                'Concavity': concavity,\n",
    "                'Convexity': convexity,\n",
    "                'DryShapeF': dry_shape_factor,\n",
    "                'WetShapeF': wet_shape_factor\n",
    "                  }\n",
    "\n",
    "pickle.dump(polygon_metrics,\n",
    "            open( '_metrics/metrics__base_metrics' + '.p', \"wb\" ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate maximum distance from the any water body\n",
    "\n",
    "if calculate_params:\n",
    "\n",
    "    maximum_edge_distance = np.zeros((len(islands),))\n",
    "    cellsize = 30\n",
    "\n",
    "    for n,i in enumerate(islands):\n",
    "\n",
    "        print n\n",
    "\n",
    "        minx, miny, maxx, maxy = i.bounds\n",
    "\n",
    "        minx = np.floor(minx) - 1 * cellsize\n",
    "        maxx = np.ceil(maxx) + 1 * cellsize\n",
    "        miny = np.floor(miny) - 1 * cellsize\n",
    "        maxy = np.ceil(maxy) + 1 * cellsize\n",
    "\n",
    "        x = np.arange(minx, maxx , cellsize)\n",
    "        y = np.arange(miny, maxy , cellsize)\n",
    "\n",
    "        mask = outline_to_mask(i.exterior, x, y)\n",
    "        distmap = morphology.distance_transform_edt(mask)\n",
    "\n",
    "        maximum_edge_distance[n] = distmap.max() * cellsize\n",
    "\n",
    "        mask = dist = None\n",
    "\n",
    "    pickle.dump(maximum_edge_distance, open( '_metrics/metrics__edge_distance' + '.p', \"wb\" ) )\n",
    "\n",
    "else:\n",
    "    \n",
    "    maximum_edge_distance = pickle.load( open( '_metrics/metrics__edge_distance.p', \"rb\" ))   \n",
    "\n",
    "    \n",
    "polygon_metrics['EdgeDist'] = maximum_edge_distance\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bounding channel width statistics for each island\n",
    "\n",
    "if calculate_params:\n",
    "\n",
    "    network_min_widths = np.zeros((len(islands),))\n",
    "    network_avg_widths = np.zeros((len(islands),))\n",
    "    network_max_widths = np.zeros((len(islands),))\n",
    "\n",
    "    for n in range(len(islands)):\n",
    "\n",
    "        i = islands[n]\n",
    "\n",
    "        # network lines off coast (for closing patches) have width 9999 - ignore\n",
    "        channels = [network_lines[b] for b in bounding_channels[n] if network_widths[b] != 9999]\n",
    "        widths = [network_widths[b] for b in bounding_channels[n] if network_widths[b] != 9999]\n",
    "        lengths = [c.length for c in channels]\n",
    "\n",
    "\n",
    "        tot_length = sum(lengths)\n",
    "        network_avg_widths[n] = sum([widths[b] * lengths[b] for b in range(len(widths))]) / tot_length\n",
    "        network_max_widths[n] = max(widths)\n",
    "        network_min_widths[n] = min(widths)\n",
    "\n",
    "    pickle.dump([network_min_widths, network_avg_widths, network_max_widths],\n",
    "                open( '_metrics/metrics__bounding_channel_widths' + '.p', \"wb\" ) )\n",
    "\n",
    "else:\n",
    "        \n",
    "    network_min_widths, network_avg_widths, network_max_widths = pickle.load( open( '_metrics/metrics__bounding_channel_widths.p', \"rb\" ))\n",
    "    \n",
    "polygon_metrics['Max_Width'] = network_max_widths\n",
    "polygon_metrics['Min_Width'] = network_min_widths\n",
    "polygon_metrics['Avg_Width'] = network_avg_widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of outflow channels\n",
    "# by counting the number of interior channels that cross the boundary of the island\n",
    "# We use patches instead of islands to avoid false positives where wide channels are\n",
    "# preserved in the outline of islands\n",
    "\n",
    "if calculate_params:\n",
    "\n",
    "    num_outflow = np.zeros((len(islands),), dtype = 'int')\n",
    "\n",
    "    for n in range(len(islands)):\n",
    "\n",
    "        lines = [i for i in interior_channels[n]]\n",
    "\n",
    "        # interior channels that touch the boundary\n",
    "        outflow = []\n",
    "\n",
    "        for l in lines:\n",
    "            if network_lines[l].intersects(patches[n].exterior):\n",
    "                outflow.append(l)\n",
    "\n",
    "        num_outflow[n] = len(outflow)\n",
    "\n",
    "    pickle.dump(num_outflow,\n",
    "                open( '_metrics/metrics__outflow_channels' + '.p', \"wb\" ) )\n",
    "\n",
    "else:\n",
    "    \n",
    "    num_outflow = pickle.load( open( '_metrics/metrics__outflow_channels.p', \"rb\" ))\n",
    "    \n",
    "polygon_metrics['NumOutflow'] = num_outflow\n",
    "\n",
    "polygon_metrics['Norm_Out'] = num_outflow / polygon_metrics['Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractal_dimension(Z, threshold=0.9):\n",
    "\n",
    "    # From https://github.com/rougier/numpy-100 (#87)\n",
    "    def boxcount(Z, k):\n",
    "        S = np.add.reduceat(\n",
    "            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
    "                               np.arange(0, Z.shape[1], k), axis=1)\n",
    "\n",
    "        # Count non-empty (0) and non-full boxes (k*k)\n",
    "        return len(np.where((S > 0) & (S < k*k))[0])\n",
    "\n",
    "    p = min(Z.shape)\n",
    "    n = 2**np.floor(np.log(p)/np.log(2))\n",
    "    n = int(np.log(n)/np.log(2))\n",
    "    sizes = 2**np.arange(n, 1, -1)\n",
    "\n",
    "    # Box counting with decreasing size\n",
    "    counts = []\n",
    "    for size in sizes:\n",
    "        counts.append(boxcount(Z, size))\n",
    "\n",
    "    # Fit the successive log(sizes) with log (counts)\n",
    "    coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n",
    "    return -coeffs[0]\n",
    "\n",
    "\n",
    "\n",
    "if calculate_params:\n",
    "\n",
    "    fractal_dimensions = np.zeros((len(islands),))\n",
    "\n",
    "    for j in range(len(islands)):\n",
    "\n",
    "        print j\n",
    "\n",
    "        outline = patches[j].exterior.simplify(15)\n",
    "        _,_,angle = Polygon_axes(outline)\n",
    "        outline = affinity.rotate(outline, angle, origin='centroid')\n",
    "\n",
    "        minx, miny, maxx, maxy = outline.bounds\n",
    "\n",
    "        cellsize = 5\n",
    "\n",
    "        if (maxy - miny > 10000) or (maxx - minx > 10000):\n",
    "            cellsize = 30\n",
    "\n",
    "        if (maxy - miny > 50000) or (maxx - minx > 50000):\n",
    "            cellsize = 60\n",
    "\n",
    "        minx = np.floor(minx) - 1 * cellsize\n",
    "        maxx = np.ceil(maxx) + 1 * cellsize\n",
    "        miny = np.floor(miny) - 1 * cellsize\n",
    "        maxy = np.ceil(maxy) + 1 * cellsize\n",
    "\n",
    "        x = np.arange(minx, maxx , cellsize)\n",
    "        y = np.arange(miny, maxy , cellsize)\n",
    "\n",
    "        mask = outline_to_mask(outline, x, y)\n",
    "        fractal_dimensions[j] = fractal_dimension(mask)\n",
    "\n",
    "        mask = None\n",
    "\n",
    "    pickle.dump(fractal_dimensions, open( '_metrics/metrics__fractal_dimensions' + '.p', \"wb\" ) )    \n",
    "    \n",
    "else:\n",
    "    \n",
    "    fractal_dimensions = pickle.load( open( '_metrics/metrics__fractal_dimensions.p', \"rb\" ))\n",
    "    \n",
    "    \n",
    "polygon_metrics['FractalD'] = fractal_dimensions\n",
    "\n",
    "pickle.dump(polygon_metrics,\n",
    "            open( '_metrics/metrics__all_metrics' + '.p', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_type = {}\n",
    "\n",
    "for k in polygon_metrics.keys():\n",
    "    \n",
    "    if polygon_metrics[k][0].dtype == 'float':\n",
    "        field_type[k] = ogr.OFTReal\n",
    "        \n",
    "    if polygon_metrics[k][0].dtype == 'int':\n",
    "        field_type[k] = ogr.OFTInteger\n",
    "\n",
    "        \n",
    "        \n",
    "create_shapefile_from_shapely_multi(islands,\n",
    "                            '_output/islands_properties.shp',\n",
    "                            fields = polygon_metrics,\n",
    "                            field_type = field_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shapefile(filename, parameters = []):\n",
    "\n",
    "\n",
    "    \n",
    "    c = fiona.open(filename)\n",
    "\n",
    "    if c[0]['geometry']['type'] == 'Polygon':\n",
    "        shp = MultiPolygon([shape(pol['geometry']) for pol in c])\n",
    "\n",
    "    elif c[0]['geometry']['type'] == 'LineString':\n",
    "        shp = MultiLineString([shape(pol['geometry']) for pol in c])\n",
    "\n",
    "    elif c[0]['geometry']['type'] == 'Point':\n",
    "        shp = MultiPoint([shape(pol['geometry']) for pol in c])\n",
    "\n",
    "    else:\n",
    "        shp = [shape(pol['geometry']) for pol in c]\n",
    "        \n",
    "        \n",
    "    \n",
    "    if parameters is 'all':\n",
    "        parameters = c[0]['properties'].keys()\n",
    "        \n",
    "    if type(parameters) is not list:\n",
    "    \n",
    "        parameters = list(parameters)\n",
    "\n",
    "\n",
    "    shp_params = {}\n",
    "\n",
    "    for param in parameters:\n",
    "        shp_params[param] = [line['properties'][param] for line in c]\n",
    "\n",
    "    c = None\n",
    "\n",
    "    return shp, shp_params\n",
    "\n",
    "def create_shapefile_from_shapely_multi(features, filename,\n",
    "                                        fields = {}, field_type = {},\n",
    "                                        buffer_width = 0, spatial_ref = 32645):\n",
    "    '''\n",
    "    Creates a shapefile from a\n",
    "    Shapely MultiPolygon, MultiLineString, or MultiPoint\n",
    "    '''\n",
    "\n",
    "\n",
    "    driver = ogr.GetDriverByName('Esri Shapefile')\n",
    "    ds = driver.CreateDataSource(filename)\n",
    "\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(spatial_ref)\n",
    "\n",
    "    layer = ds.CreateLayer('', srs, ogr.wkbPolygon)\n",
    "\n",
    "    for f in fields.keys():\n",
    "        fieldDefn = ogr.FieldDefn(f, field_type[f])\n",
    "        layer.CreateField(fieldDefn)\n",
    "\n",
    "    defn = layer.GetLayerDefn()\n",
    "\n",
    "\n",
    "    for i in range(len(features)):\n",
    "\n",
    "        poly = features[i].buffer(buffer_width)\n",
    "\n",
    "        # Create a new feature (attribute and geometry)\n",
    "        feat = ogr.Feature(defn)\n",
    "\n",
    "        for f in fields.keys():\n",
    "            feat.SetField(f, fields[f][i])\n",
    "\n",
    "        # Make a geometry from Shapely object\n",
    "        geom = ogr.CreateGeometryFromWkb(poly.wkb)\n",
    "        feat.SetGeometry(geom)\n",
    "\n",
    "        layer.CreateFeature(feat)\n",
    "        feat = geom = None  # destroy these\n",
    "\n",
    "\n",
    "    # Save and close everything\n",
    "    ds = layer = feat = geom = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_shapefile_filepath = '_output/islands_properties.shp'\n",
    "islands, parameters = load_shapefile(properties_shapefile_filepath, parameters='all')\n",
    "\n",
    "parameters = pickle.load( open( '_metrics/metrics__all_metrics.p', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_parameters = pd.DataFrame()\n",
    "\n",
    "\n",
    "for k in parameters.keys():\n",
    "    \n",
    "    params = np.array(parameters[k])\n",
    "    \n",
    "    if params.min() == 0:\n",
    "        vals = np.log10(params + 0.1)\n",
    "        vals[vals == -1] = 0\n",
    "    \n",
    "    else:\n",
    "        vals = np.log10(params / params.min())\n",
    "    \n",
    "    normalized_parameters[k.encode('utf-8')] = vals\n",
    "\n",
    "cols = ['Area',#'CH_Area','EdgeDist',\n",
    "#          'Perimeter','WetPerim','CH_Perim',\n",
    "         'AspectR',#'MajorAxis','MinorAxis',\n",
    "         'DryShapeF',#'WetShapeF',\n",
    "        'FractalD',\n",
    "#         'Norm_Out',\n",
    "         'Min_Width','Max_Width','Avg_Width',\n",
    "        'NumOutflow',\n",
    "#         'Concavity',\n",
    "        #'Circular',\n",
    "        'Solidity','Convexity',\n",
    "       ]\n",
    "    #                                  'P_Eq_Diam','Eq_A_Diam']\n",
    "    \n",
    "    \n",
    "normalized_parameters = normalized_parameters[cols]\n",
    "    \n",
    "scaled_data = MinMaxScaler().fit_transform(normalized_parameters.values)\n",
    "scaled_parameters = pd.DataFrame(data = scaled_data, columns = normalized_parameters.columns)\n",
    "\n",
    "pickle.dump(scaled_parameters,\n",
    "            open( '_metrics/metrics__scaled_parameters' + '.p', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_type = {}\n",
    "\n",
    "for k in scaled_parameters.keys():\n",
    "    field_type[k] = ogr.OFTReal\n",
    "       \n",
    "\n",
    "create_shapefile_from_shapely_multi(islands,\n",
    "                                    '_output/islands_properties_scaled.shp',\n",
    "                                    fields = scaled_parameters,\n",
    "                                    field_type = field_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from http://www.ritchieng.com/machine-learning-project-customer-segments/\n",
    "def pca_results_funct(good_data, pca, plot = False):\n",
    "    '''\n",
    "    Create a DataFrame of the PCA results\n",
    "    Includes dimension feature weights and explained variance\n",
    "    Visualizes the PCA results\n",
    "    '''\n",
    "\n",
    "    # Dimension indexing\n",
    "    dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n",
    "\n",
    "    # PCA components\n",
    "    components = pd.DataFrame(np.round(pca.components_, 4), columns = good_data.keys())\n",
    "    components.index = dimensions\n",
    "\n",
    "    # PCA explained variance\n",
    "    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n",
    "    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n",
    "    variance_ratios.index = dimensions\n",
    "\n",
    "    if plot:\n",
    "        \n",
    "        # Create a bar plot visualization\n",
    "        fig, ax = plt.subplots(figsize = (14,8))\n",
    "\n",
    "        # Plot the feature weights as a function of the components\n",
    "        components.plot(ax = ax, kind = 'bar');\n",
    "        ax.set_ylabel(\"Feature Weights\")\n",
    "        ax.set_xticklabels(dimensions, rotation=0)\n",
    "\n",
    "\n",
    "        # Display the explained variance ratios\n",
    "        for i, ev in enumerate(pca.explained_variance_ratio_):\n",
    "            ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n",
    "\n",
    "    # Return a concatenated DataFrame\n",
    "    return pd.concat([variance_ratios, components], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 95.02444744714879 %\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "\n",
    "'''Principal component analysis (PCA) is a statistical procedure\n",
    "that uses an orthogonal transformation to convert a set of\n",
    "observations of possibly correlated variables into a set of\n",
    "values of linearly uncorrelated variables called principal components\n",
    "'''\n",
    "\n",
    "noise = 0\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "pca.fit(scaled_parameters + noise)\n",
    "\n",
    "# TODO: Transform the good data using the PCA fit above\n",
    "reduced_data = pca.transform(scaled_parameters + noise)\n",
    "\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data)\n",
    "reduced_data.columns = ['PCA' + str(k) for k in reduced_data.columns.values if type(k) <> 'str']\n",
    "\n",
    "pd.to_pickle(reduced_data, '_PCA/pca__reduced_data.p')\n",
    "\n",
    "pca_results = pca_results_funct(scaled_parameters + noise, pca)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print 'Explained variance:', sum(explained_variance)*100, '%'\n",
    "\n",
    "# pd.to_pickle(explained_variance,'_PCA/pca__explained_variance.p')\n",
    "\n",
    "field_type = {}\n",
    "\n",
    "for k in reduced_data.keys():\n",
    "    field_type[k] = ogr.OFTReal\n",
    "\n",
    "pca_filename = '_output/pca_dimensions.shp'\n",
    "\n",
    "create_shapefile_from_shapely_multi(islands,\n",
    "                                    pca_filename,\n",
    "                                    fields = reduced_data,\n",
    "                                    field_type = field_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_geosom(data):\n",
    "    \n",
    "    for k in c_data.Y.keys():\n",
    "        c_data.Y[k] = [k] + list(data[k,:])\n",
    "\n",
    "    c_data.cluster(\"geoSom_poly\", params, alphaType='linear',\n",
    "                   nRows=nRows, nCols=nCols, wType=\"queen\",\n",
    "                   iters = 1000, num_neighboring_neurons = 2,\n",
    "                  fileName = '_PCA/' + str(nRows) + 'x' + str(nCols)\n",
    "                  )\n",
    "    c_data.exportArcData(clusterpy_output_dir + '/classes_' + directory)\n",
    "    c_data.exportOutputs(clusterpy_output_dir + '/output_' + directory + \".csv\")\n",
    "\n",
    "    c_data.outputCluster = {'r2a': [], 'r2aRoot': []}\n",
    "    \n",
    "    \n",
    "    return np.array(c_data.region2areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'geoSom_poly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3edcb09d87b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnRows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0misland_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shapefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_clusterpy/classes_20x20.shp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geoSom_poly'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shapefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_PCA/20x20.shp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'iter999'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8d47c453bdd2>\u001b[0m in \u001b[0;36mload_shapefile\u001b[0;34m(filename, parameters)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mshp_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'properties'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'geoSom_poly'"
     ]
    }
   ],
   "source": [
    "reduced_data = pickle.load( open( '_PCA/pca__reduced_data.p', \"rb\" ))\n",
    "\n",
    "nRows, nCols = 20,20\n",
    "\n",
    "island_classes, class_num = load_shapefile('_clusterpy/classes_20x20.shp', parameters=['geoSom_poly'])\n",
    "neurons, count = load_shapefile('_PCA/20x20.shp', parameters=['ID','iter999'])\n",
    "\n",
    "\n",
    "\n",
    "count_gt_zero = [n for n,l in enumerate(count['iter999']) if l>0]\n",
    "\n",
    "count = np.array(count['iter999']).astype('int').reshape((nRows,nCols))\n",
    "label_count = [i for i in count.flatten() if i > 0]\n",
    "\n",
    "unique_classes = np.unique(class_num['geoSom_poly'])\n",
    "all_classes = np.array(class_num['geoSom_poly'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_classes = np.zeros((len(all_classes),), dtype = 'int')\n",
    "\n",
    "for n,i in enumerate(unique_classes):\n",
    "    \n",
    "    loc = all_classes == i\n",
    "    new_classes[loc] = count_gt_zero[n]\n",
    "\n",
    "all_classes = new_classes.copy()\n",
    "\n",
    "mean_vals = pd.DataFrame(index = np.unique(all_classes))\n",
    "\n",
    "for c in reduced_data.columns:\n",
    "    \n",
    "    for n,l in enumerate(np.unique(all_classes)):\n",
    "        \n",
    "        loc = list(np.where(all_classes == l)[0])\n",
    "        \n",
    "        mean_vals.loc[l,c] = np.mean(reduced_data.loc[loc,c])\n",
    "\n",
    "mean_vals = mean_vals.fillna(0)\n",
    "\n",
    "all_classes = new_classes.copy()\n",
    "unique_classes = np.unique(all_classes)\n",
    "\n",
    "U_matrix = np.ones(((nRows*2) - 1, (nCols*2) - 1)) * 9999\n",
    "labels = np.arange(nRows * nCols).reshape((nRows,nCols))\n",
    "labels[count == 0] = -1\n",
    "\n",
    "\n",
    "labels[count == 0] = -1\n",
    "n_labels = labels.copy()\n",
    "n_labels[labels > -1] = range(len(unique_classes))\n",
    "\n",
    "distances = squareform(pdist(mean_vals, metric = 'euclidean'))\n",
    "distances_pd = pd.DataFrame(data = distances, index = unique_classes, columns = unique_classes)\n",
    "\n",
    "neurons2 = MultiPolygon([i for n,i in enumerate(neurons) if count.flatten()[n] > 0.0])\n",
    "W = ps.weights.Queen.from_iterable(neurons2)\n",
    "neighbors = W.neighbors\n",
    "\n",
    "n_neighbors = {}\n",
    "for k in neighbors.keys():\n",
    "    n_neighbors[labels[n_labels == k][0]] = [labels[n_labels == i][0] for i in neighbors[k]]\n",
    "\n",
    "    \n",
    "    \n",
    "U_matrix = np.ones(((nRows*2) - 1, (nCols*2) - 1)) * 9999\n",
    "\n",
    "for k in n_neighbors.keys():\n",
    "\n",
    "    row, col = np.where(labels == k)\n",
    "    i = row[0]\n",
    "    j = col[0]\n",
    "    row_ind = (i * 2)\n",
    "    col_ind = (j * 2)\n",
    "\n",
    "    neigh = n_neighbors[k]\n",
    "\n",
    "    val_sum = 0\n",
    "    val_count = 0\n",
    "\n",
    "    if len(neigh) > 0:\n",
    "        for n in neigh:\n",
    "\n",
    "            rown, coln = np.where(labels == n)\n",
    "            i1 = rown[0]\n",
    "            j1 = coln[0]\n",
    "\n",
    "            row_ind1 = (i1 * 2)\n",
    "            col_ind1 = (j1 * 2)\n",
    "\n",
    "            row_ind_mid = row_ind + (i1 - i)\n",
    "            col_ind_mid = col_ind + (j1 - j)\n",
    "\n",
    "            U_matrix[row_ind_mid, col_ind_mid] = distances_pd.loc[k,n]\n",
    "            val_sum += U_matrix[row_ind_mid, col_ind_mid]\n",
    "            val_count += 1.\n",
    "\n",
    "\n",
    "        U_matrix[row_ind, col_ind] = val_sum / val_count\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        U_matrix[row_ind, col_ind] = 9999\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_matrix_masked = np.ma.masked_where(U_matrix > 100 , U_matrix)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(U_matrix_masked, vmin = 0, vmax = 1.5, cmap = 'Greys')\n",
    "plt.colorbar()\n",
    "\n",
    "for n,k in enumerate(labels.flatten()):\n",
    "    \n",
    "    if k > -1:\n",
    "\n",
    "        row, col = np.where(labels == k)\n",
    "        j = row[0]\n",
    "        i = col[0]\n",
    "\n",
    "        plt.scatter(i*2, j*2, c='r', s = count.flatten()[n] * 6)\n",
    "\n",
    "        \n",
    "msizes = np.linspace(1,16,4) * 5\n",
    "\n",
    "l1 = plt.scatter([],[], c='r', s = msizes[0])\n",
    "l2 = plt.scatter([],[], c='r', s = msizes[1])\n",
    "l3 = plt.scatter([],[], c='r', s = msizes[2])\n",
    "l4 = plt.scatter([],[], c='r', s = msizes[3])\n",
    "\n",
    "legend_labels = ['1', '6', '11', '16']\n",
    "\n",
    "leg = plt.legend([l1, l2, l3, l4], legend_labels, ncol=1, frameon=True, fontsize=12,\n",
    "handlelength=2, loc = 1, borderpad = 0.8,\n",
    "handletextpad=0, title='Islands per cluster', scatterpoints = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(X,model,mode='l2'):\n",
    "    distances = []\n",
    "    weights = []\n",
    "    children=model.children_\n",
    "    dims = (X.shape[1],1)\n",
    "    distCache = {}\n",
    "    weightCache = {}\n",
    "    for childs in children:\n",
    "        c1 = X[childs[0]].reshape(dims)\n",
    "        c2 = X[childs[1]].reshape(dims)\n",
    "        c1Dist = 0\n",
    "        c1W = 1\n",
    "        c2Dist = 0\n",
    "        c2W = 1\n",
    "        if childs[0] in distCache.keys():\n",
    "            c1Dist = distCache[childs[0]]\n",
    "            c1W = weightCache[childs[0]]\n",
    "        if childs[1] in distCache.keys():\n",
    "            c2Dist = distCache[childs[1]]\n",
    "            c2W = weightCache[childs[1]]\n",
    "        d = np.linalg.norm(c1-c2)\n",
    "        cc = ((c1W*c1)+(c2W*c2))/(c1W+c2W)\n",
    "\n",
    "        X = np.vstack((X,cc.T))\n",
    "\n",
    "        newChild_id = X.shape[0]-1\n",
    "\n",
    "        # How to deal with a higher level cluster merge with lower distance:\n",
    "        if mode=='l2':  # Increase the higher level cluster size suing an l2 norm\n",
    "            added_dist = (c1Dist**2+c2Dist**2)**0.5 \n",
    "            dNew = (d**2 + added_dist**2)**0.5\n",
    "        elif mode == 'max':  # If the previrous clusters had higher distance, use that one\n",
    "            dNew = max(d,c1Dist,c2Dist)\n",
    "        elif mode == 'actual':  # Plot the actual distance.\n",
    "            dNew = d\n",
    "\n",
    "\n",
    "        wNew = (c1W + c2W)\n",
    "        distCache[newChild_id] = dNew\n",
    "        weightCache[newChild_id] = wNew\n",
    "\n",
    "        distances.append(dNew)\n",
    "        weights.append( wNew)\n",
    "    return distances, weights\n",
    "\n",
    "connectivity = W.full()[0]\n",
    "\n",
    "\n",
    "\n",
    "Agg_c5 = cluster.AgglomerativeClustering(n_clusters = 5, compute_full_tree=True,\n",
    "                                      connectivity = connectivity, affinity='euclidean', linkage='ward',)  \n",
    "AggLabels5 = Agg_c5.fit_predict(mean_vals.values)\n",
    "\n",
    "\n",
    "Agg_c14 = cluster.AgglomerativeClustering(n_clusters = 14, compute_full_tree=True,\n",
    "                                      connectivity = connectivity, affinity='euclidean', linkage='ward',)  \n",
    "AggLabels14 = Agg_c14.fit_predict(mean_vals.values)\n",
    "\n",
    "Agg_c15 = cluster.AgglomerativeClustering(n_clusters = 15, compute_full_tree=True,\n",
    "                                      connectivity = connectivity, affinity='euclidean', linkage='ward',)  \n",
    "AggLabels15 = Agg_c15.fit_predict(mean_vals.values)\n",
    "\n",
    "\n",
    "Agg_c30 = cluster.AgglomerativeClustering(n_clusters = 30, compute_full_tree=True,\n",
    "                                      connectivity = connectivity, affinity='euclidean', linkage='ward',)  \n",
    "AggLabels30 = Agg_c30.fit_predict(mean_vals.values)\n",
    "\n",
    "distance, weight = get_distances(mean_vals.values, Agg_c14, mode='l2')\n",
    "linkage_matrix = np.column_stack([Agg_c14.children_, distance, weight]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {}\n",
    "\n",
    "\n",
    "for k in range(5,31,1):\n",
    "    \n",
    "    Agg = cluster.AgglomerativeClustering(n_clusters = k, compute_full_tree=True, connectivity = connectivity, affinity='euclidean', linkage='ward')  \n",
    "    l = Agg.fit_predict(mean_vals.values) \n",
    "    \n",
    "    \n",
    "    c = np.zeros((len(island_classes),), dtype = 'int')\n",
    "    \n",
    "    for n,i in enumerate(unique_classes):\n",
    "        \n",
    "        loc = np.where(all_classes == i)[0]\n",
    "        c[loc] = l[n]\n",
    "        \n",
    "    \n",
    "    fields[str(k)] = c\n",
    "\n",
    "    \n",
    "fields['id'] = range(len(island_classes))\n",
    "\n",
    "field_type = {}\n",
    "for k in fields.keys():\n",
    "    field_type[str(k)] = ogr.OFTInteger\n",
    "\n",
    "create_shapefile_from_shapely_multi(island_classes,\n",
    "                                    '_output/agg_classes.shp',\n",
    "                                    fields = fields,\n",
    "                                    field_type = field_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Agg_c12 = cluster.AgglomerativeClustering(n_clusters = 12, compute_full_tree=True,\n",
    "                                      connectivity = connectivity, affinity='euclidean', linkage='ward',)  \n",
    "AggLabels12 = Agg_c12.fit_predict(mean_vals.values)\n",
    "\n",
    "distance, weight = get_distances(mean_vals.values, Agg_c12, mode='l2')\n",
    "linkage_matrix = np.column_stack([Agg_c12.children_, distance, weight]).astype(float)\n",
    "\n",
    "\n",
    "den = dendrogram(linkage_matrix, no_plot = True)\n",
    "\n",
    "\n",
    "cluster_idxs = defaultdict(list)\n",
    "for c, pi in zip(den['color_list'], den['icoord']):\n",
    "    for leg in pi[1:3]:\n",
    "        i = (leg - 5.0) / 10.0\n",
    "        if abs(i - int(i)) < 1e-5:\n",
    "            cluster_idxs[c].append(int(i))\n",
    "\n",
    "\n",
    "\n",
    "dendrogram_labels = []\n",
    "for i in den['leaves']:\n",
    "    dendrogram_labels.append(list(np.where(all_classes == i)[0]))\n",
    "\n",
    "\n",
    "temp = {den[\"leaves\"][ii]: dendrogram_labels[ii] for ii in range(len(den[\"leaves\"]))}\n",
    "def llf(xx):\n",
    "#     return \"({})\".format(temp[xx])\n",
    "    return \"{}\".format('')\n",
    "\n",
    "color_dict12 = {0: 'deepskyblue', 1: 'darksalmon', 2: 'silver', 3: 'orange', 4: 'orangered',\n",
    "               5: 'paleturquoise', 6:'violet', 7:'rosybrown', 8:'thistle', 9:'slateblue',\n",
    "               10: 'darkorchid', 11:'palegreen'}\n",
    "\n",
    "dflt_col = \"#808080\" \n",
    "link_cols = {}\n",
    "for i, i12 in enumerate(linkage_matrix[:,:2].astype(int)):\n",
    "    \n",
    "    c1, c2 = (link_cols[x] if x > len(linkage_matrix) else color_dict12[AggLabels12[x]]\n",
    "    for x in i12)\n",
    "    link_cols[i+1+len(linkage_matrix)] = c1 if c1 == c2 else dflt_col\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5,25))\n",
    "den = dendrogram(linkage_matrix,\n",
    "                 leaf_label_func=llf,\n",
    "#                  link_color_func = lambda k: new_color_list[k],\n",
    "                 link_color_func=lambda x: link_cols[x],\n",
    "                leaf_font_size=6,\n",
    "                orientation = 'left'\n",
    "                )\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "\n",
    "no_spine = {'left': True, 'bottom': True, 'right': True, 'top': True}\n",
    "sns.despine(**no_spine);\n",
    "\n",
    "# labels5 = [[i for ni,i in enumerate(AggLabels14[den['leaves']]) if AggLabels5[den['leaves']][ni] == n] for n in [1,2,3,4,0]]\n",
    "# dendrogram_labels_color = []\n",
    "# colors = ['deepskyblue', 'g', 'orange', 'rebeccapurple', 'indianred']\n",
    "# for n,i in enumerate(color_sublist):\n",
    "#     dendrogram_labels_color += [colors[n]] * len(labels5[n])\n",
    "    \n",
    "# dendrogram_labels_color = [color_dict14[AggLabels14[x]] for x in den['leaves']]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.yaxis.tick_right()\n",
    "# ylbls = ax.get_ymajorticklabels()\n",
    "# for n,y in enumerate(ylbls):\n",
    "#     y.set_color(dendrogram_labels_color[n])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "dx = 0\n",
    "dy = 0.05\n",
    "offset = matplotlib.transforms.ScaledTranslation(dx,dy, fig.dpi_scale_trans)\n",
    "\n",
    "# apply offset transform to all x ticklabels.\n",
    "for label in ax.yaxis.get_majorticklabels():\n",
    "    label.set_transform(label.get_transform() + offset)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('_test/agg_test12_vertical.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_U_matrix_sized_labels(labels, thin = False):\n",
    "\n",
    "    U_matrix_labels = np.zeros(((nRows*2) - 1, (nCols*2) - 1))\n",
    "    U_matrix_labels_diff = np.zeros(((nRows*2) - 1, (nCols*2) - 1))\n",
    "\n",
    "    U_matrix_labels[0::2, 0::2] = labels + 1\n",
    "\n",
    "    U_matrix_labels_diff[0::2, 1::2] = np.diff(labels)\n",
    "\n",
    "    for i in range(0,nCols*2 - 2,2):\n",
    "\n",
    "        right_cell = U_matrix_labels_diff[0::2, i+1]\n",
    "        U_matrix_labels[0::2, i+1][right_cell == 0] = U_matrix_labels[0::2, i][right_cell == 0]\n",
    "\n",
    "\n",
    "    U_matrix_labels_diff[1::2,:] = np.diff(U_matrix_labels[0::2,:], axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(0,nRows*2 - 2,2):\n",
    "\n",
    "        bottom_cell = U_matrix_labels_diff[i+1, 0::2]\n",
    "        U_matrix_labels[i+1, 0::2][bottom_cell == 0] = U_matrix_labels[i, 0::2][bottom_cell == 0]\n",
    "\n",
    "\n",
    "    # for i in range(0,nCols - 1,2):\n",
    "\n",
    "    #     U_matrix_labels_diff[1::2, i*2+1] = labels14[:-1,i] - labels14[1:,i+1]\n",
    "\n",
    "\n",
    "    # NW\n",
    "    for i in range(1,nCols):\n",
    "\n",
    "        for j in range(1,nRows):\n",
    "\n",
    "            l1 = U_matrix_labels[j*2, i*2]\n",
    "            l2 = U_matrix_labels[j*2 - 2, i*2 - 2]\n",
    "\n",
    "            if (l1 == l2) & (l1 > 0):\n",
    "                if thin:\n",
    "                    if (U_matrix_labels[j*2, i*2 - 2] == l1) & (U_matrix_labels[j*2 - 2, i*2] == l1):\n",
    "                        U_matrix_labels[j*2 - 1, i*2 - 1] = l1\n",
    "                else:\n",
    "                    U_matrix_labels[j*2 - 1, i*2 - 1] = l1\n",
    "\n",
    "\n",
    "\n",
    "    # SE            \n",
    "    for i in range(0,nCols-1):\n",
    "\n",
    "        for j in range(1,nRows):\n",
    "\n",
    "            l1 = U_matrix_labels[j*2, i*2]\n",
    "            l2 = U_matrix_labels[j*2 - 2, i*2 + 2]\n",
    "\n",
    "            if (l1 == l2) & (l1 > 0):\n",
    "                if thin:\n",
    "                    if (U_matrix_labels[j*2, i*2 + 2] == l1) & (U_matrix_labels[j*2 - 2, i*2] == l1):\n",
    "                        U_matrix_labels[j*2 - 1, i*2 + 1] = l1\n",
    "                else: \n",
    "                    U_matrix_labels[j*2 - 1, i*2 + 1] = l1\n",
    "    \n",
    "    return U_matrix_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.features import shapes as rasterio_shapes\n",
    "from shapely.geometry import shape\n",
    "from descartes import PolygonPatch\n",
    "import matplotlib as mpl\n",
    "from shapely.affinity import translate\n",
    "from matplotlib.patches import Polygon\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "color_dict12 = {0: 'deepskyblue', 1: 'darksalmon', 2: 'moccasin', 3: 'orange', 4: 'orangered',\n",
    "               5: 'paleturquoise', 6:'violet', 7:'rosybrown', 8:'thistle', 9:'slateblue',\n",
    "               10: 'darkorchid', 11:'palegreen'}\n",
    "\n",
    "labels12 = np.zeros((400,)) -1\n",
    "labels12[labels.flatten() != -1] = AggLabels12\n",
    "\n",
    "labels12 = labels12.reshape(20,20)\n",
    "\n",
    "\n",
    "image = np.arange(0,39 * 39).reshape(39,39).astype('int16')\n",
    "# image[U_matrix > 100] = 9999 \n",
    "results = (\n",
    "{'properties': {'raster_val': v}, 'geometry': s}\n",
    "for i, (s, v) \n",
    "in enumerate(\n",
    "    rasterio_shapes(image)))\n",
    "\n",
    "geoms = list(results)\n",
    "\n",
    "label12_shapes_all = MultiPolygon([shape(i['geometry']) for i in geoms])\n",
    "label12_shapes_all = translate(label12_shapes_all, xoff=-0.5, yoff=-0.5)\n",
    "\n",
    "\n",
    "# U_matrix_labels5 = get_U_matrix_sized_labels(labels5, thin = True)\n",
    "\n",
    "\n",
    "# image = U_matrix_labels5.astype('int16')\n",
    "# results = (\n",
    "# {'properties': {'raster_val': v}, 'geometry': s}\n",
    "# for i, (s, v) \n",
    "# in enumerate(\n",
    "#     rasterio_shapes(image)))\n",
    "\n",
    "# geoms = list(results)\n",
    "\n",
    "# label5_shapes = MultiPolygon([shape(i['geometry']) for i in geoms if i['properties']['raster_val'] > -1])\n",
    "# label5_shapes_number = [int(i['properties']['raster_val'])-1 for i in geoms if i['properties']['raster_val'] > -1]\n",
    "\n",
    "# label5_shapes = translate(label5_shapes, xoff=-0.5, yoff=-0.5)\n",
    "\n",
    "U_matrix_labels = get_U_matrix_sized_labels(labels12)\n",
    "# # cheating\n",
    "# U_matrix_labels[27,1] = 8.\n",
    "# U_matrix_labels[37,33] = 2.\n",
    "# U_matrix_labels[11,11] = 11.\n",
    "# U_matrix_labels[29,31] = 9.\n",
    "\n",
    "\n",
    "label_count12 = {}\n",
    "\n",
    "for k in np.unique(labels12):\n",
    "    \n",
    "    if k > -1:\n",
    "        \n",
    "        orig_labels = labels[labels12 == k]\n",
    "\n",
    "        n = 0\n",
    "        for i in orig_labels:\n",
    "            n += np.sum(all_classes == i)\n",
    "\n",
    "        label_count12[k] = n\n",
    "\n",
    "\n",
    "image = U_matrix_labels.astype('int16')\n",
    "results = (\n",
    "{'properties': {'raster_val': v}, 'geometry': s}\n",
    "for i, (s, v) \n",
    "in enumerate(\n",
    "    rasterio_shapes(image)))\n",
    "\n",
    "geoms = list(results)\n",
    "\n",
    "label12_shapes = MultiPolygon([shape(i['geometry']) for i in geoms if i['properties']['raster_val'] > 0])\n",
    "label12_shapes_number = [int(i['properties']['raster_val'])-1 for i in geoms if i['properties']['raster_val'] > 0]\n",
    "\n",
    "label12_shapes = translate(label12_shapes, xoff=-0.5, yoff=-0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "U_matrix_masked = np.ma.masked_where(U_matrix > 100 , U_matrix)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(9, 5))\n",
    "\n",
    "im = ax1.imshow(U_matrix_masked, cmap = 'Greys', vmin = 0, vmax = 1.5)\n",
    "\n",
    "for n,k in enumerate(labels.flatten()):\n",
    "    \n",
    "    if k > -1:\n",
    "\n",
    "        row, col = np.where(labels == k)\n",
    "        j = row[0]\n",
    "        i = col[0]\n",
    "\n",
    "        ax1.scatter(i*2, j*2, c='r', s = count.flatten()[n] * 3, alpha = 0.9)\n",
    "\n",
    "        \n",
    "msizes = np.linspace(1 * 3, count.max() * 3,4)\n",
    "\n",
    "l1 = ax1.scatter([],[], c='r', s = msizes[0])\n",
    "l2 = ax1.scatter([],[], c='r', s = msizes[1])\n",
    "l3 = ax1.scatter([],[], c='r', s = msizes[2])\n",
    "l4 = plt.scatter([],[], c='r', s = msizes[3])\n",
    "\n",
    "legend_labels = [str(int((i) / 3.)) for i in msizes]\n",
    "\n",
    "leg = ax1.legend([l1, l2, l3, l4], legend_labels, ncol=1, frameon=True, fontsize=10,\n",
    "handlelength=2, loc = 1, borderpad = 0.3,\n",
    "handletextpad=0, title='Islands\\nin cluster', scatterpoints = 1)\n",
    "\n",
    "patches = [PolygonPatch(feature, edgecolor='0.6',\n",
    "                        facecolor=\"None\", linewidth=0.5, alpha = 0.1) for n,feature in enumerate(label12_shapes_all)]\n",
    "ax1.add_collection(mpl.collections.PatchCollection(patches, match_original=True))\n",
    "\n",
    "\n",
    "selected_patches = range(12)\n",
    "\n",
    "patches = [PolygonPatch(feature, edgecolor=color_dict12[label12_shapes_number[n]],\n",
    "                        facecolor=\"None\", linewidth=1, alpha = 1) for n,feature in enumerate(label12_shapes)\n",
    "                         if label12_shapes_number[n] in selected_patches]\n",
    "ax1.add_collection(mpl.collections.PatchCollection(patches, match_original=True))\n",
    "\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['left'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "\n",
    "ax1.axes.get_xaxis().set_visible(False)\n",
    "ax1.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax1.set_xlim(-1, 40)\n",
    "ax1.set_ylim(40, -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "U_matrix_labels = get_U_matrix_sized_labels(labels12)\n",
    "# # cheating\n",
    "# U_matrix_labels[27,1] = 8.\n",
    "# U_matrix_labels[37,33] = 2.\n",
    "# U_matrix_labels[11,11] = 11.\n",
    "# U_matrix_labels[29,31] = 9.\n",
    "\n",
    "\n",
    "label_count12 = {}\n",
    "\n",
    "for k in np.unique(labels12):\n",
    "    \n",
    "    if k > -1:\n",
    "        \n",
    "        orig_labels = labels[labels12 == k]\n",
    "\n",
    "        n = 0\n",
    "        for i in orig_labels:\n",
    "            n += np.sum(all_classes == i)\n",
    "\n",
    "        label_count12[k] = n\n",
    "\n",
    "\n",
    "image = U_matrix_labels.astype('int16')\n",
    "results = (\n",
    "{'properties': {'raster_val': v}, 'geometry': s}\n",
    "for i, (s, v) \n",
    "in enumerate(\n",
    "    rasterio_shapes(image)))\n",
    "\n",
    "geoms = list(results)\n",
    "\n",
    "label12_shapes = MultiPolygon([shape(i['geometry']) for i in geoms if i['properties']['raster_val'] > 0])\n",
    "label12_shapes_number = [int(i['properties']['raster_val'])-1 for i in geoms if i['properties']['raster_val'] > 0]\n",
    "\n",
    "label12_shapes = translate(label12_shapes, xoff=-0.5, yoff=-0.5)\n",
    "\n",
    "\n",
    "\n",
    "im = ax2.imshow(U_matrix_masked, cmap = 'Greys', vmin = 0, vmax = 1.5)\n",
    "# plt.colorbar(im, ax=ax2, orientation=\"horizontal\", pad=0)\n",
    "\n",
    "\n",
    "\n",
    "selected_patches = range(14)\n",
    "\n",
    "patches = [PolygonPatch(feature, edgecolor='0.6',\n",
    "                        facecolor=\"None\", linewidth=0.5, alpha = 0.1) for n,feature in enumerate(label12_shapes_all)]\n",
    "ax2.add_collection(mpl.collections.PatchCollection(patches, match_original=True))\n",
    "\n",
    "patches = [PolygonPatch(feature, edgecolor=color_dict12[label12_shapes_number[n]],\n",
    "                        facecolor=\"None\", linewidth=1, alpha = 1) for n,feature in enumerate(label12_shapes)\n",
    "                         if label12_shapes_number[n] in selected_patches]\n",
    "ax2.add_collection(mpl.collections.PatchCollection(patches, match_original=True))\n",
    "\n",
    "patches = [PolygonPatch(feature, edgecolor=color_dict12[label12_shapes_number[n]],\n",
    "                        facecolor=color_dict12[label12_shapes_number[n]], linewidth=1, alpha = 0.2) for n,feature in enumerate(label12_shapes)\n",
    "                         if label12_shapes_number[n] in selected_patches]\n",
    "ax2.add_collection(mpl.collections.PatchCollection(patches, match_original=True))\n",
    "\n",
    "\n",
    "\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "ax2.axes.get_xaxis().set_visible(False)\n",
    "ax2.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax2.set_xlim(-1, 40)\n",
    "ax2.set_ylim(40, -1)\n",
    "\n",
    "\n",
    "plt.tight_layout(h_pad = -2)\n",
    "\n",
    "plt.savefig('_test/U_matrices.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "islands, parameters = load_shapefile('_output/agg_classes.shp', parameters=['12'])\n",
    "AggClasses = np.array(parameters['12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: 'Tidal 1', 1: 'Transitional 2', 2: 'Inactive', 3: 'Fluvial 1', 4: 'Fluvial 2',\n",
    "               5: 'Tidal 2', 6:'Estuarine 2', 7:'Transitional 1', 8:'Estuarine 3', 9:'Estuarine 4',\n",
    "               10: 'Estuarine 1', 11:'Other', -1:''}\n",
    "\n",
    "groups = {2: [0,5], #Tidal\n",
    "         1:[6,8,9,10], #Estuarine\n",
    "         3: [3,4], #Fluvial\n",
    "         5:[2], #Inactive\n",
    "         4:[1,7], #Transitional\n",
    "         6:[11]} #Other\n",
    "\n",
    "\n",
    "labels = {2: 'Tidal',\n",
    "         1: 'Estuarine',\n",
    "         3: 'Fluvial',\n",
    "         5: 'Inactive',\n",
    "         4: 'Transitional',\n",
    "         6:'Other'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_labels = np.zeros_like(AggClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in groups.keys():\n",
    "    \n",
    "    for value in groups[key]:\n",
    "        \n",
    "        grouped_labels[AggClasses == value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_parameters = pickle.load( open( '_metrics/metrics__scaled_parameters' + '.p', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "'''\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "The final classification scheme uses\n",
    "threshold = 0.8\n",
    "distance = 0.3\n",
    "\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "'''\n",
    "\n",
    "groups = [10,6,8,9,-1,0,5,-1,3,4,-1,7,1,-1,2,-1,11]\n",
    "\n",
    "\n",
    "cmap = plt.cm.gray\n",
    "norm = mpl.colors.Normalize(vmin=-1, vmax=0)\n",
    "\n",
    "r = 0.3\n",
    "# cols = [i for i in data.columns if i != 'class']\n",
    "\n",
    "data = scaled_parameters.copy()\n",
    "\n",
    "\n",
    "# for col in data.columns:\n",
    "    \n",
    "#     data[col] = data[col] - (data[col].mean() - 0.5)\n",
    "\n",
    "\n",
    "\n",
    "data['class'] = AggClasses\n",
    "ioi_classes =  groups\n",
    "cols = scaled_parameters.columns\n",
    "cols_clean = ['Area', 'Aspect Ratio', 'Dry Shape\\nFactor', 'Fractal\\nDimension', 'Minimum\\nChannel Width',\n",
    "       'Maximum\\nChannel Width', 'Average\\nChannel Width', 'Number of\\nOutflow Channels', 'Solidity', 'Convexity']\n",
    "# islands_of_interest = [252, 624, 694, 808, 1027, 1005, 1000, 1053, 210, 193]\n",
    "\n",
    "# ioi_classes_all = [data.loc[i,'class'] for i in islands_of_interest]\n",
    "# ioi_classes = np.unique(ioi_classes_all)\n",
    "\n",
    "\n",
    "# ioi_classes = [0,3,6,4,7,1,2,16,9]\n",
    "# fig, axes = plt.subplots(nrows=len(cols), ncols=1, figsize=(6, 19))\n",
    "\n",
    "vals_all = []\n",
    "\n",
    "\n",
    "plt.figure(figsize = (5, 13))\n",
    "gs1 = gridspec.GridSpec(len(cols),1)\n",
    "gs1.update(wspace=0.075, hspace=0.05, left=0.1, right=0.95, bottom = .1, top = 0.98) # set the spacing between axes. \n",
    "\n",
    "axes = [plt.subplot(gs1[i]) for i in range(len(cols))]\n",
    "\n",
    "for nn,col in enumerate(cols):\n",
    "\n",
    "    axes[nn].set_facecolor('1')\n",
    "\n",
    "    vals = [list(data.loc[data['class'] == i, col]- 0.5) if i>-1 else 0 for i in ioi_classes]\n",
    "    vals_med = [np.median(i) for i in vals]\n",
    "\n",
    "    vals_all.append(vals_med)\n",
    "\n",
    "    parts = axes[nn].violinplot(vals[:-1], range(len(ioi_classes)-1), points=20, widths=0.7,\n",
    "                          showmeans=False, showextrema=True, showmedians=True,);\n",
    "\n",
    "    \n",
    "    axes[nn].scatter(np.ones_like(vals[-1]) * len(ioi_classes)-1, vals[-1], c = 'k', marker = 'x', lw = 1)\n",
    "    \n",
    "    for n,pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor('w')#cmap(norm(vals_med[n])))\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(1)\n",
    "        pc.set_linewidth(1)\n",
    "\n",
    "    for c in parts.keys():\n",
    "        if c != 'bodies':\n",
    "            vp = parts[c]\n",
    "            vp.set_edgecolor('black')\n",
    "            vp.set_linewidth(1)\n",
    "\n",
    "    axes[nn].axhline(0, color='black', lw = 0.5)\n",
    "#     axes[nn].plot(line_range, np.ones_like(line_range) * 0.5, 'k--', lw = 1, zorder = 1)\n",
    "    axes[nn].set_ylim(-0.6,0.6)\n",
    "    axes[nn].tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "    axes[nn].set_xticks(np.unique(data['class']))\n",
    "#     axes[nn].grid(color='0.5', linestyle='--', linewidth=1)\n",
    "#     axes[nn].axis(False);\n",
    "#     axes[nn].spines['top'].set_visible(False)\n",
    "#     axes[nn].spines['right'].set_visible(False)\n",
    "#     axes[nn].spines['bottom'].set_visible(False)\n",
    "#     axes[nn].spines['left'].set_visible(False)\n",
    "\n",
    "    h = axes[nn].set_ylabel(cols_clean[nn], labelpad = 5,\n",
    "                            horizontalalignment = 'center',\n",
    "                            verticalalignment = 'bottom')\n",
    "    h.set_rotation(90)\n",
    "    axes[nn].set_xlim(-0.5,len(ioi_classes) - 0.5);#data['class'].max() + 0.5)\n",
    "\n",
    "axes[nn].tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=True)\n",
    "axes[nn].set_xticks(np.arange(len(ioi_classes)))\n",
    "axes[nn].set_xticklabels([label_dict[i] for i in ioi_classes], rotation = 90);\n",
    "# axes[nn].set_xlabel('Island classes')\n",
    "\n",
    "\n",
    "fig.subplots_adjust(left=0.3)\n",
    "fig.subplots_adjust(right=0.95)\n",
    "\n",
    "plt.savefig('_figures/violin_single_groups.png', dpi = 150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_type = {}\n",
    "\n",
    "for k in scaled_parameters.keys():\n",
    "    field_type[k] = ogr.OFTReal\n",
    "\n",
    "filename = '_output/scaled_parameters.shp'\n",
    "\n",
    "create_shapefile_from_shapely_multi(islands,\n",
    "                                    filename,\n",
    "                                    fields = scaled_parameters,\n",
    "                                    field_type = field_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_classes =  range(0,12)\n",
    "\n",
    "KL = pd.DataFrame(columns=data.columns[:-1], index = [i for i in ioi_classes if i > -1])\n",
    "\n",
    "for col in data.columns[:-1]:\n",
    "    \n",
    "    for i in ioi_classes:\n",
    "        \n",
    "        if i > -1:\n",
    "\n",
    "            d0 = data.loc[data['class'] == i, col]\n",
    "            # d0 = d0 / d0.sum()\n",
    "            d1 = data.loc[data['class'] != i, col]\n",
    "            # d1 = d1 / d1.sum()\n",
    "\n",
    "            count_d0, bins = np.histogram(d0, bins = 20, range = (0,1), density = True)\n",
    "            count_d1, bins = np.histogram(d1, bins = 20, range = (0,1), density = True)\n",
    "\n",
    "            KL.loc[i, col] = entropy(count_d0 / count_d0.sum() + 0.000001, count_d1 / count_d1.sum() + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "KLo = KL.values.astype('float').T.copy()\n",
    "\n",
    "KLo[KLo < 1] = 0\n",
    "\n",
    "mpl.rcParams.update(inline_rc)\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('bone_r')\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (7,5))\n",
    "\n",
    "\n",
    "img = ax.imshow(KLo, cmap = cmap, vmin = 0, vmax = 5, aspect = 1)\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(img, cmap = cmap)\n",
    "cbar.set_label('KL divergence')\n",
    "\n",
    "# corr = scaled_parameters.corr().values\n",
    "\n",
    "ax.set_xticks(range(KLo.shape[1]))\n",
    "ax.set_yticks(range(KLo.shape[0]))\n",
    "\n",
    "ax.grid(which='minor', color='k', linestyle='-', linewidth=0.5, alpha = 0.5)\n",
    "ax.set_xticks(np.arange(-.5, KLo.shape[1], 1), minor=True);\n",
    "ax.set_yticks(np.arange(-.5, KLo.shape[0], 1), minor=True);\n",
    "\n",
    "\n",
    "label_dict = {0: 'Tidal 1', 1: 'Transitional 2', 2: 'Inactive', 3: 'Fluvial 1', 4: 'Fluvial 2',\n",
    "               5: 'Tidal 2', 6:'Estuarine 2', 7:'Transitional 1', 8:'Estuarine 3', 9:'Estuarine 4',\n",
    "               10: 'Estuarine 1', 11:'Other', -1:''}\n",
    "\n",
    "parameter_labels = ['Area',#'Area of Convex Hull','Max. Distance from Edge',\n",
    "                    #'Perimeter','Wet Perimeter','Perimeter of Convex Hull',\n",
    "                    'Aspect Ratio',#'Major Axis','Minor Axis',\n",
    "                    'Dry Shape Factor',#'Wet Shape Factor',\n",
    "                    'Fractal Dimension',\n",
    "                    'Min. Channel Width','Max. Channel Width','Avg. Channel Width','Number of\\nOutflow Channels',\n",
    "                    #'Concavity',\n",
    "#                     'Circularity',\n",
    "                    'Solidity','Convexity',]\n",
    "\n",
    "ax.set_xticklabels([label_dict[i] for i in KL.index], rotation = 90);\n",
    "ax.set_yticklabels(parameter_labels);\n",
    "\n",
    "ax.set_ylim(-0.5,9.5)\n",
    "\n",
    "ax.vlines(3.5, -1, 12, linewidth = 2)\n",
    "ax.vlines(5.5, -1, 12, linewidth = 2)\n",
    "ax.vlines(7.5, -1, 12, linewidth = 2)\n",
    "ax.vlines(9.5, -1, 12, linewidth = 2)\n",
    "ax.vlines(10.5, -1, 12, linewidth = 2)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "fig.subplots_adjust(top=0.98)\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "fig.subplots_adjust(left=0.25)\n",
    "fig.subplots_adjust(right=1)\n",
    "\n",
    "plt.savefig('_figures/KLdivergence.png', dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = scaled_parameters.copy()\n",
    "\n",
    "# for col in data.columns:\n",
    "    \n",
    "#     data[col] = data[col] - (data[col].mean() - 0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data['class'] = grouped_labels\n",
    "ioi_classes =  range(1,7)\n",
    "\n",
    "for i in ioi_classes:\n",
    "\n",
    "    if i > -1:\n",
    "\n",
    "        d0 = data.loc[data['class'] == i, data.columns[:-1]]\n",
    "        # d0 = d0 / d0.sum()\n",
    "        d1 = data.loc[data['class'] != i, data.columns[:-1]]\n",
    "        # d1 = d1 / d1.sum()\n",
    "\n",
    "        count_d0, bins = np.histogram(d0, bins = 20, range = (0,1), density = True)\n",
    "        count_d1, bins = np.histogram(d1, bins = 20, range = (0,1), density = True)\n",
    "\n",
    "        print entropy(count_d0 / count_d0.sum() + 0.000001, count_d1 / count_d1.sum() + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_order = [10,6,9,8,2,3,4,11,5,0,7,1]\n",
    "class_order5 = [0,0,0,0,0, 4,4, 3, 2, 1,1,1,1,1]\n",
    "\n",
    "ind_for_labels12 = {}\n",
    "\n",
    "labels = np.arange(nRows * nCols).reshape((nRows,nCols))\n",
    "labels[count == 0] = -1\n",
    "\n",
    "for k in range(12):\n",
    "\n",
    "    loc = np.where(labels12.flatten() == k)[0]\n",
    "    classes_k = labels.flatten()[loc]\n",
    "\n",
    "    all_classes_k = []\n",
    "\n",
    "    for ck in classes_k:\n",
    "\n",
    "        loc1 = list(np.where(all_classes == ck)[0])\n",
    "        all_classes_k += loc1\n",
    "\n",
    "    ind_for_labels12[k] = all_classes_k\n",
    "    \n",
    "    \n",
    "    \n",
    "ind_for_labels40 = []\n",
    "class_order40 = []\n",
    "\n",
    "for k in class_order:\n",
    "\n",
    "    loc = np.where(labels12.flatten() == k)[0]\n",
    "    classes_k = labels.flatten()[loc]\n",
    "    class_order40.append(list(classes_k))\n",
    "\n",
    "\n",
    "    for ck in classes_k:\n",
    "\n",
    "        loc1 = list(np.where(all_classes == ck)[0])\n",
    "\n",
    "        ind_for_labels40.append(loc1)\n",
    "    \n",
    "\n",
    "class_order40_flat = [i for j in class_order40 for i in j]\n",
    "    \n",
    "mean_scaled_parameters_40 = pd.DataFrame(index = class_order40_flat, columns = scaled_parameters.columns)\n",
    "\n",
    "percentile_scaled_parameters_40 = pd.DataFrame(index = class_order40_flat, columns = scaled_parameters.columns)\n",
    "\n",
    "for k in class_order40_flat:\n",
    "    \n",
    "    \n",
    "    loc1 = list(np.where(all_classes == k)[0])\n",
    "    \n",
    "    vals = scaled_parameters.loc[loc1]\n",
    "    mean_scaled_parameters_40.loc[k] = vals.median().values\n",
    "    \n",
    "    \n",
    "    \n",
    "for c in scaled_parameters.columns:\n",
    "    \n",
    "    percentile_scaled_parameters_40[c] = [percentileofscore(scaled_parameters[c], i) for i in mean_scaled_parameters_40[c]]\n",
    "\n",
    "    \n",
    "# c = [[i] * len(class_order40[class_order[n]]) for n,i in enumerate(class_order5)]\n",
    "# mean_scaled_parameters_40['class'] = [i for j in c for i in j]\n",
    "\n",
    "# c = [[i] * len(class_order40[i]) for n,i in enumerate(class_order)]\n",
    "# mean_scaled_parameters_40['class'] = [i for j in c for i in j]\n",
    "\n",
    "\n",
    "heatmap40 = percentile_scaled_parameters_40.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,1, figsize=(6, 9))\n",
    "\n",
    "ax.imshow(heatmap40, vmin = 0, vmax = 100, aspect = 0.1, cmap = 'Spectral_r')\n",
    "\n",
    "\n",
    "for i in [ax]:\n",
    "\n",
    "    i.spines['right'].set_visible(False)\n",
    "    i.spines['top'].set_visible(False)\n",
    "    i.spines['left'].set_visible(False)\n",
    "    i.spines['bottom'].set_visible(False)\n",
    "\n",
    "    i.axes.get_xaxis().set_visible(False)\n",
    "    i.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "c = [len(AggLabels12[AggLabels12 == i]) for i in class_order]\n",
    "bounds = np.cumsum(c)\n",
    "\n",
    "for b in bounds[:-1]:\n",
    "    \n",
    "    ax.axhline(b-0.6, ls = ':', linewidth = 1, color = 'k')\n",
    "    \n",
    "ax.set_xlim(-1,18)\n",
    "    \n",
    "    \n",
    "plt.savefig('_test/meanvals_40.png', dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_labels = ['Area',#'Area of Convex Hull','Max. Distance from Edge',\n",
    "                    #'Perimeter','Wet Perimeter','Perimeter of Convex Hull',\n",
    "                    'Aspect Ratio',#'Major Axis','Minor Axis',\n",
    "                    'Dry Shape Factor',#'Wet Shape Factor',\n",
    "                    'Fractal Dimension',\n",
    "                    'Min. Channel Width','Max. Channel Width','Avg. Channel Width','Number of Outflow Channels',\n",
    "                    #'Concavity',\n",
    "#                     'Circularity',\n",
    "                    'Solidity','Convexity',]\n",
    "#                     'Perimeter Equivalent Diameter','Equivalent Area Diameter']\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "pca_corr = pca_results[scaled_parameters.columns.values]\n",
    "\n",
    "pd.to_pickle(pca_corr, '_PCA/pca_correlations.p')\n",
    "\n",
    "ax = sns.heatmap(pca_corr,\n",
    "            vmin=-1, vmax = 1,\n",
    "            cmap = 'bwr_r',\n",
    "            cbar_kws={'label': 'Variable weight', 'ticks': [-1,0,1]},\n",
    "            xticklabels = parameter_labels)\n",
    "ax.set_yticklabels(range(1,11),rotation = 0);\n",
    "ax.set_ylabel('Dimension')\n",
    "\n",
    "ax.axhline(y=0, color='k',linewidth=1)\n",
    "ax.axhline(y=pca_corr.shape[0], color='k',linewidth=1)\n",
    "ax.axvline(x=0, color='k',linewidth=1)\n",
    "ax.axvline(x=pca_corr.shape[1], color='k',linewidth=1)\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left = 0.08, right = 1.05, top = 0.95, bottom = 0.5)\n",
    "\n",
    "plt.savefig('_test/pca__variable_weight.png', dpi = 300, )    \n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "plt.grid(zorder=1, alpha = 0.25)\n",
    "\n",
    "plt.bar(range(len(explained_variance)),explained_variance*100, zorder=2)\n",
    "plt.plot(range(len(explained_variance)),explained_variance*100, 'ko-', zorder=3)\n",
    "\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Percentage of explained variances')\n",
    "plt.ylim(0,np.ceil(np.max(explained_variance*10))*10)\n",
    "plt.xticks(range(0,len(explained_variance)), range(1,len(explained_variance) + 1))\n",
    "\n",
    "clabels = ['{:.1%}'.format(i) for i in explained_variance]\n",
    "clabels = [i if i[0]<>'0' else i[1:] for i in clabels]\n",
    "\n",
    "\n",
    "for label, x, y in zip(clabels, range(len(explained_variance)), explained_variance*100):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-2,8),\n",
    "        textcoords='offset points', ha='left', va='center', size=10)\n",
    "    \n",
    "plt.savefig('_test/pca__scree_plot.png', dpi = 300)    \n",
    "plt.close()\n",
    "\n",
    "mpl.rcParams.update(inline_rc)\n",
    "\n",
    "norm = matplotlib.colors.Normalize(vmin = -1.5, vmax = 1.5)\n",
    "cmap = matplotlib.cm.get_cmap('bwr_r')\n",
    "\n",
    "\n",
    "axes = pd.plotting.scatter_matrix(scaled_parameters,\n",
    "                                  alpha = 0.1, figsize = (12,12),\n",
    "                                  diagonal = 'kde', c = 'k',\n",
    "                                  density_kwds={'color': 'k'});\n",
    "\n",
    "corr = scaled_parameters.corr().values\n",
    "\n",
    "\n",
    "for i,j in zip(*plt.np.triu_indices_from(axes, k=1)):\n",
    "    \n",
    "    axes[j,i].clear()\n",
    "    axes[j,i].set_ylabel(parameter_labels[j], rotation = 0, horizontalalignment = 'right', verticalalignment = 'center')\n",
    "    axes[j,i].set_xticks([])\n",
    "    axes[j,i].set_yticks([])\n",
    "    axes[j,i].set_facecolor(cmap(norm(corr[j,i])))\n",
    "    axes[j,i].annotate(\"%.2f\" %corr[j,i], (0.5, 0.5), xycoords='axes fraction', ha='center', va='center')\n",
    "\n",
    "axes[0,0].set_xticks([])\n",
    "axes[0,0].set_yticks([])\n",
    "axes[-1,-1].set_xticks([])\n",
    "axes[-1,-1].set_yticks([])\n",
    "\n",
    "\n",
    "j = -1\n",
    "for i in range(len(parameter_labels)):\n",
    "    axes[j,i].set_xlabel(parameter_labels[i], rotation = 90, horizontalalignment = 'left', verticalalignment = 'top')\n",
    "\n",
    "    \n",
    "i,j = 0,0\n",
    "axes[j,i].set_ylabel(parameter_labels[j], rotation = 0, horizontalalignment = 'right', verticalalignment = 'center')\n",
    "\n",
    "i,j = -1,-1\n",
    "axes[j,i].set_ylabel(parameter_labels[j], rotation = 0, horizontalalignment = 'right', verticalalignment = 'center')\n",
    "\n",
    "plt.subplots_adjust(right = 0.95, left = 0.2, top = 0.95)\n",
    "\n",
    "plt.savefig('_test/pca__variable_correlation_matrix.png', dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dropped = {}\n",
    "noise = 0\n",
    "\n",
    "for col in range(len(scaled_parameters.columns)):\n",
    "    \n",
    "    \n",
    "    columns = list(scaled_parameters.columns.values)\n",
    "    dropped_col = columns.pop(col)\n",
    "    \n",
    "    print dropped_col\n",
    "    \n",
    "    n, m = 20,20\n",
    "\n",
    "    directory = dropped_col + '_' + str(n) + 'x' + str(m)\n",
    "    print directory\n",
    "    \n",
    "    n = 4\n",
    "    while n < 7:\n",
    "    \n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(scaled_parameters[columns] + noise)\n",
    "\n",
    "        # TODO: Transform the good data using the PCA fit above\n",
    "        reduced_data = pca.transform(scaled_parameters[columns] + noise)\n",
    "\n",
    "\n",
    "        # Create a DataFrame for the reduced data\n",
    "        reduced_data = pd.DataFrame(reduced_data)\n",
    "        reduced_data.columns = ['PCA' + str(k) for k in reduced_data.columns.values if type(k) <> 'str']\n",
    "\n",
    "        # pd.to_pickle(reduced_data, '_PCA/pca__reduced_data.p')\n",
    "\n",
    "        pca_results = pca_results_funct(scaled_parameters[columns] + noise, pca)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        print 'Explained variance:', sum(explained_variance)*100, '%'\n",
    "        \n",
    "        if sum(explained_variance)*100 < 95:\n",
    "            n += 1\n",
    "        else:\n",
    "            n = 10\n",
    "    \n",
    "    \n",
    "\n",
    "    # reduced_data = pickle.load( open( '_PCA/pca__reduced_data.p', \"rb\" ))\n",
    "\n",
    "    nRows, nCols = 20,20\n",
    "\n",
    "    island_classes, class_num = load_shapefile('_clusterpy/classes_' + directory + '.shp', parameters=['geoSom_poly'])\n",
    "    neurons, count = load_shapefile('_PCA/' + directory + '.shp', parameters=['ID','iter999'])\n",
    "\n",
    "\n",
    "\n",
    "    count_gt_zero = [n for n,l in enumerate(count['iter999']) if l>0]\n",
    "\n",
    "    count = np.array(count['iter999']).astype('int').reshape((nRows,nCols))\n",
    "    label_count = [i for i in count.flatten() if i > 0]\n",
    "\n",
    "    unique_classes = np.unique(class_num['geoSom_poly'])\n",
    "    all_classes = np.array(class_num['geoSom_poly'])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    neurons2 = MultiPolygon([i for n,i in enumerate(neurons) if count.flatten()[n] > 0.0])\n",
    "    W = ps.weights.Queen.from_iterable(neurons2)\n",
    "    connectivity = W.full()[0]\n",
    "\n",
    "\n",
    "\n",
    "    new_classes = np.zeros((len(all_classes),), dtype = 'int')\n",
    "\n",
    "    for n,i in enumerate(unique_classes):\n",
    "\n",
    "        loc = all_classes == i\n",
    "        new_classes[loc] = count_gt_zero[n]\n",
    "\n",
    "    all_classes = new_classes.copy()\n",
    "\n",
    "    mean_vals = pd.DataFrame(index = np.unique(all_classes))\n",
    "\n",
    "    for c in reduced_data.columns:\n",
    "\n",
    "        for n,l in enumerate(np.unique(all_classes)):\n",
    "\n",
    "            loc = list(np.where(all_classes == l)[0])\n",
    "\n",
    "            mean_vals.loc[l,c] = np.mean(reduced_data.loc[loc,c])\n",
    "\n",
    "    mean_vals = mean_vals.fillna(0)\n",
    "\n",
    "\n",
    "    Agg_c12 = cluster.AgglomerativeClustering(n_clusters = 12, compute_full_tree=True,\n",
    "                                          connectivity = connectivity, affinity='euclidean', linkage='ward',)  \n",
    "    AggLabels12d = Agg_c12.fit_predict(mean_vals.values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    c = np.zeros((len(island_classes),), dtype = 'int')\n",
    "    \n",
    "    for n,i in enumerate(unique_classes):\n",
    "        \n",
    "        loc = np.where(all_classes == i)[0]\n",
    "        c[loc] = AggLabels12d[n]\n",
    "        \n",
    "    \n",
    "    labels_dropped[dropped_col] = c\n",
    "\n",
    "    \n",
    "    \n",
    "dropped_cols = ['channels','channels_MaxW', 'channels_MinW', 'channels_AvgW']\n",
    "    \n",
    "columns_s = [['AspectR','DryShapeF','FractalD','NumOutflow','Solidity','Convexity'],\n",
    "             ['Max_Width','AspectR','DryShapeF','FractalD','NumOutflow','Solidity','Convexity'],\n",
    "             ['Min_Width','AspectR','DryShapeF','FractalD','NumOutflow','Solidity','Convexity'],\n",
    "             ['Avg_Width','AspectR','DryShapeF','FractalD','NumOutflow','Solidity','Convexity']]\n",
    "\n",
    "for ii in range(4):\n",
    "\n",
    "\n",
    "    dropped_col = dropped_cols[ii]\n",
    "\n",
    "    columns = columns_s[ii]\n",
    "\n",
    "    print \"Dropped col:\", dropped_col\n",
    "\n",
    "    n, m = 20,20\n",
    "\n",
    "    directory = dropped_col + '_' + str(n) + 'x' + str(m)\n",
    "    print \"Directory: \", directory\n",
    "\n",
    "\n",
    "\n",
    "    n = 4\n",
    "    while n < 7:\n",
    "\n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(scaled_parameters[columns] + noise)\n",
    "\n",
    "        # TODO: Transform the good data using the PCA fit above\n",
    "        reduced_data = pca.transform(scaled_parameters[columns] + noise)\n",
    "\n",
    "\n",
    "        # Create a DataFrame for the reduced data\n",
    "        reduced_data = pd.DataFrame(reduced_data)\n",
    "        reduced_data.columns = ['PCA' + str(k) for k in reduced_data.columns.values if type(k) <> 'str']\n",
    "\n",
    "        # pd.to_pickle(reduced_data, '_PCA/pca__reduced_data.p')\n",
    "\n",
    "        pca_results = pca_results_funct(scaled_parameters[columns] + noise, pca)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        print 'Explained variance:', sum(explained_variance)*100, '%'\n",
    "\n",
    "        if sum(explained_variance)*100 < 95:\n",
    "            n += 1\n",
    "        else:\n",
    "            n = 10\n",
    "\n",
    "    nRows, nCols = 20,20\n",
    "\n",
    "    if dropped_col == 'channels_AvgW':\n",
    "        \n",
    "        island_classes, class_num = load_shapefile('_clusterpy/classes_' + directory + '.shp', parameters=['geoSom_2019'])\n",
    "        unique_classes = np.unique(class_num['geoSom_2019'])\n",
    "        all_classes = np.array(class_num['geoSom_2019'])\n",
    "    if dropped_col != 'channels_AvgW':\n",
    "        island_classes, class_num = load_shapefile('_clusterpy/classes_' + directory + '.shp', parameters=['geoSom_poly'])\n",
    "        unique_classes = np.unique(class_num['geoSom_poly'])\n",
    "        all_classes = np.array(class_num['geoSom_poly'])\n",
    "\n",
    "    neurons, count = load_shapefile('_PCA/' + directory + '.shp', parameters=['ID','iter999'])\n",
    "\n",
    "\n",
    "\n",
    "    count_gt_zero = [n for n,l in enumerate(count['iter999']) if l>0]\n",
    "\n",
    "    count = np.array(count['iter999']).astype('int').reshape((nRows,nCols))\n",
    "    label_count = [i for i in count.flatten() if i > 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    neurons2 = MultiPolygon([i for n,i in enumerate(neurons) if count.flatten()[n] > 0.0])\n",
    "    W = ps.weights.Queen.from_iterable(neurons2)\n",
    "    connectivity = W.full()[0]\n",
    "\n",
    "\n",
    "\n",
    "    new_classes = np.zeros((len(all_classes),), dtype = 'int')\n",
    "\n",
    "    for n,i in enumerate(unique_classes):\n",
    "\n",
    "        loc = all_classes == i\n",
    "        new_classes[loc] = count_gt_zero[n]\n",
    "\n",
    "    all_classes = new_classes.copy()\n",
    "\n",
    "    mean_vals = pd.DataFrame(index = np.unique(all_classes))\n",
    "\n",
    "    for c in reduced_data.columns:\n",
    "\n",
    "        for n,l in enumerate(np.unique(all_classes)):\n",
    "\n",
    "            loc = list(np.where(all_classes == l)[0])\n",
    "\n",
    "            mean_vals.loc[l,c] = np.mean(reduced_data.loc[loc,c])\n",
    "\n",
    "    mean_vals = mean_vals.fillna(0)\n",
    "\n",
    "\n",
    "    Agg_c12 = cluster.AgglomerativeClustering(n_clusters = 12, compute_full_tree=True,\n",
    "                                          connectivity = connectivity, affinity='euclidean', linkage='ward',)  \n",
    "    AggLabels12d = Agg_c12.fit_predict(mean_vals.values)\n",
    "\n",
    "\n",
    "\n",
    "    c = np.zeros((len(island_classes),), dtype = 'int')\n",
    "\n",
    "    for n,i in enumerate(unique_classes):\n",
    "\n",
    "        loc = np.where(all_classes == i)[0]\n",
    "        c[loc] = AggLabels12d[n]\n",
    "\n",
    "\n",
    "    labels_dropped[dropped_col] = c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
