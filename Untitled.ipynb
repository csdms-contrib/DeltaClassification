{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusterPy: Library of spatially constrained clustering algorithms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python2.7/site-packages/pysal/__init__.py:65: VisibleDeprecationWarning: PySAL's API will be changed on 2018-12-31. The last release made with this API is version 1.14.4. A preview of the next API version is provided in the `pysal` 2.0 prelease candidate. The API changes and a guide on how to change imports is provided at https://pysal.org/about\n",
      "  ), VisibleDeprecationWarning)\n",
      "/anaconda3/envs/ml/lib/python2.7/site-packages/numba/decorators.py:33: NumbaDeprecationWarning: \u001b[1mautojit is deprecated, use jit instead, which provides the same functionality. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numba-autojit\u001b[0m\n",
      "  warnings.warn(NumbaDeprecationWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "from osgeo import ogr, gdal, osr\n",
    "import fiona\n",
    "\n",
    "from shapely.geometry import shape, Point, Polygon, MultiLineString, MultiPoint, MultiPolygon, LineString\n",
    "from shapely import affinity\n",
    "\n",
    "from scipy.ndimage import morphology\n",
    "\n",
    "from utilities import *\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "\n",
    "from osgeo import ogr, gdal, osr\n",
    "import fiona\n",
    "from shapely.geometry import shape, Point, Polygon, MultiLineString, MultiPoint, MultiPolygon, LineString\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "inline_rc = dict(mpl.rcParams)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import clusterpy\n",
    "\n",
    "\n",
    "import itertools\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "\n",
    "import pysal as ps\n",
    "from sklearn import cluster\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "# %pylab inline\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram, set_link_color_palette\n",
    "# from fastcluster import linkage\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import rgb2hex, colorConverter\n",
    "# from matplotlib.pyplot import *\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-195a7ade9b8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#range(len(scaled_parameters.columns)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdropped_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaled_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "def predict_geosom(data):\n",
    "    \n",
    "    for k in c_data.Y.keys():\n",
    "        c_data.Y[k] = [k] + list(data[k,:])\n",
    "\n",
    "    c_data.cluster(\"geoSom_poly\", params, alphaType='linear',\n",
    "                   nRows=nRows, nCols=nCols, wType=\"queen\",\n",
    "                   iters = 1000, num_neighboring_neurons = 1,\n",
    "                  fileName = '_PCA/' + directory\n",
    "                  )\n",
    "    c_data.exportArcData(clusterpy_output_dir + '/classes_' + directory)\n",
    "    c_data.exportOutputs(clusterpy_output_dir + '/output_' + directory + \".csv\")\n",
    "\n",
    "    c_data.outputCluster = {'r2a': [], 'r2aRoot': []}\n",
    "    \n",
    "    \n",
    "    return np.array(c_data.region2areas)\n",
    "\n",
    "# labels_dropped = {}\n",
    "\n",
    "for col in [0]:#range(len(scaled_parameters.columns)):\n",
    "\n",
    "    columns = list(scaled_parameters.columns.values)\n",
    "    dropped_col = columns.pop(col)\n",
    "    \n",
    "    dropped_col = 'channels_AvgW'\n",
    "    \n",
    "    columns = ['Avg_Width',\n",
    "        'AspectR',\n",
    "     'DryShapeF',\n",
    "     'FractalD',\n",
    "     'NumOutflow',\n",
    "     'Solidity',\n",
    "     'Convexity']\n",
    "    \n",
    "    \n",
    "    \n",
    "    print dropped_col\n",
    "\n",
    "    # PCA\n",
    "\n",
    "    '''Principal component analysis (PCA) is a statistical procedure\n",
    "    that uses an orthogonal transformation to convert a set of\n",
    "    observations of possibly correlated variables into a set of\n",
    "    values of linearly uncorrelated variables called principal components\n",
    "    '''\n",
    "\n",
    "    noise = 0\n",
    "\n",
    "    n = 4\n",
    "    while n < 7:\n",
    "    \n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(scaled_parameters[columns] + noise)\n",
    "\n",
    "        # TODO: Transform the good data using the PCA fit above\n",
    "        reduced_data = pca.transform(scaled_parameters[columns] + noise)\n",
    "\n",
    "\n",
    "        # Create a DataFrame for the reduced data\n",
    "        reduced_data = pd.DataFrame(reduced_data)\n",
    "        reduced_data.columns = ['PCA' + str(k) for k in reduced_data.columns.values if type(k) <> 'str']\n",
    "\n",
    "        # pd.to_pickle(reduced_data, '_PCA/pca__reduced_data.p')\n",
    "\n",
    "        pca_results = pca_results_funct(scaled_parameters[columns] + noise, pca)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        print 'Explained variance:', sum(explained_variance)*100, '%'\n",
    "        \n",
    "        if sum(explained_variance)*100 < 95:\n",
    "            n += 1\n",
    "        else:\n",
    "            n = 10\n",
    "\n",
    "    # pd.to_pickle(explained_variance,'_PCA/pca__explained_variance.p')\n",
    "\n",
    "    field_type = {}\n",
    "\n",
    "    for k in reduced_data.keys():\n",
    "        field_type[k] = ogr.OFTReal\n",
    "\n",
    "    pca_filename = '_output/' + dropped_col + '_pca_dimensions.shp'\n",
    "\n",
    "    create_shapefile_from_shapely_multi(islands,\n",
    "                                        pca_filename,\n",
    "                                        fields = reduced_data,\n",
    "                                        field_type = field_type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clusterpy_output_dir = '_clusterpy'\n",
    "    c_data = clusterpy.importArcData(pca_filename[:-4])\n",
    "    params = reduced_data.columns.tolist()\n",
    "\n",
    "    n, m = 20,20\n",
    "\n",
    "    directory = dropped_col + '_' + str(n) + 'x' + str(m)\n",
    "    print directory\n",
    "\n",
    "    nRows = n\n",
    "    nCols = m\n",
    "    regions = predict_geosom(reduced_data.values)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # reduced_data = pickle.load( open( '_PCA/pca__reduced_data.p', \"rb\" ))\n",
    "\n",
    "    nRows, nCols = 20,20\n",
    "\n",
    "    island_classes, class_num = load_shapefile('_clusterpy/classes_' + directory + '.shp', parameters=['geoSom_poly'])\n",
    "    neurons, count = load_shapefile('_PCA/' + directory + '.shp', parameters=['ID','iter999'])\n",
    "\n",
    "\n",
    "\n",
    "    count_gt_zero = [n for n,l in enumerate(count['iter999']) if l>0]\n",
    "\n",
    "    count = np.array(count['iter999']).astype('int').reshape((nRows,nCols))\n",
    "    label_count = [i for i in count.flatten() if i > 0]\n",
    "\n",
    "    unique_classes = np.unique(class_num['geoSom_poly'])\n",
    "    all_classes = np.array(class_num['geoSom_poly'])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    neurons2 = MultiPolygon([i for n,i in enumerate(neurons) if count.flatten()[n] > 0.0])\n",
    "    W = ps.weights.Queen.from_iterable(neurons2)\n",
    "    connectivity = W.full()[0]\n",
    "\n",
    "\n",
    "\n",
    "    new_classes = np.zeros((len(all_classes),), dtype = 'int')\n",
    "\n",
    "    for n,i in enumerate(unique_classes):\n",
    "\n",
    "        loc = all_classes == i\n",
    "        new_classes[loc] = count_gt_zero[n]\n",
    "\n",
    "    all_classes = new_classes.copy()\n",
    "\n",
    "    mean_vals = pd.DataFrame(index = np.unique(all_classes))\n",
    "\n",
    "    for c in reduced_data.columns:\n",
    "\n",
    "        for n,l in enumerate(np.unique(all_classes)):\n",
    "\n",
    "            loc = list(np.where(all_classes == l)[0])\n",
    "\n",
    "            mean_vals.loc[l,c] = np.mean(reduced_data.loc[loc,c])\n",
    "\n",
    "    mean_vals = mean_vals.fillna(0)\n",
    "\n",
    "\n",
    "    Agg_c12 = cluster.AgglomerativeClustering(n_clusters = 12, compute_full_tree=True,\n",
    "                                          connectivity = connectivity, affinity='euclidean', linkage='ward',)  \n",
    "    AggLabels12d = Agg_c12.fit_predict(mean_vals.values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    c = np.zeros((len(island_classes),), dtype = 'int')\n",
    "    \n",
    "    for n,i in enumerate(unique_classes):\n",
    "        \n",
    "        loc = np.where(all_classes == i)[0]\n",
    "        c[loc] = AggLabels12d[n]\n",
    "        \n",
    "    \n",
    "    labels_dropped[dropped_col] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dropped = {}\n",
    "noise = 0\n",
    "\n",
    "for col in range(len(scaled_parameters.columns)):\n",
    "    \n",
    "    \n",
    "    columns = list(scaled_parameters.columns.values)\n",
    "    dropped_col = columns.pop(col)\n",
    "    \n",
    "    print dropped_col\n",
    "    \n",
    "    n, m = 20,20\n",
    "\n",
    "    directory = dropped_col + '_' + str(n) + 'x' + str(m)\n",
    "    print directory\n",
    "    \n",
    "    n = 4\n",
    "    while n < 7:\n",
    "    \n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(scaled_parameters[columns] + noise)\n",
    "\n",
    "        # TODO: Transform the good data using the PCA fit above\n",
    "        reduced_data = pca.transform(scaled_parameters[columns] + noise)\n",
    "\n",
    "\n",
    "        # Create a DataFrame for the reduced data\n",
    "        reduced_data = pd.DataFrame(reduced_data)\n",
    "        reduced_data.columns = ['PCA' + str(k) for k in reduced_data.columns.values if type(k) <> 'str']\n",
    "\n",
    "        # pd.to_pickle(reduced_data, '_PCA/pca__reduced_data.p')\n",
    "\n",
    "        pca_results = pca_results_funct(scaled_parameters[columns] + noise, pca)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        print 'Explained variance:', sum(explained_variance)*100, '%'\n",
    "        \n",
    "        if sum(explained_variance)*100 < 95:\n",
    "            n += 1\n",
    "        else:\n",
    "            n = 10\n",
    "    \n",
    "    \n",
    "\n",
    "    # reduced_data = pickle.load( open( '_PCA/pca__reduced_data.p', \"rb\" ))\n",
    "\n",
    "    nRows, nCols = 20,20\n",
    "\n",
    "    island_classes, class_num = load_shapefile('_clusterpy/classes_' + directory + '.shp', parameters=['geoSom_poly'])\n",
    "    neurons, count = load_shapefile('_PCA/' + directory + '.shp', parameters=['ID','iter999'])\n",
    "\n",
    "\n",
    "\n",
    "    count_gt_zero = [n for n,l in enumerate(count['iter999']) if l>0]\n",
    "\n",
    "    count = np.array(count['iter999']).astype('int').reshape((nRows,nCols))\n",
    "    label_count = [i for i in count.flatten() if i > 0]\n",
    "\n",
    "    unique_classes = np.unique(class_num['geoSom_poly'])\n",
    "    all_classes = np.array(class_num['geoSom_poly'])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    neurons2 = MultiPolygon([i for n,i in enumerate(neurons) if count.flatten()[n] > 0.0])\n",
    "    W = ps.weights.Queen.from_iterable(neurons2)\n",
    "    connectivity = W.full()[0]\n",
    "\n",
    "\n",
    "\n",
    "    new_classes = np.zeros((len(all_classes),), dtype = 'int')\n",
    "\n",
    "    for n,i in enumerate(unique_classes):\n",
    "\n",
    "        loc = all_classes == i\n",
    "        new_classes[loc] = count_gt_zero[n]\n",
    "\n",
    "    all_classes = new_classes.copy()\n",
    "\n",
    "    mean_vals = pd.DataFrame(index = np.unique(all_classes))\n",
    "\n",
    "    for c in reduced_data.columns:\n",
    "\n",
    "        for n,l in enumerate(np.unique(all_classes)):\n",
    "\n",
    "            loc = list(np.where(all_classes == l)[0])\n",
    "\n",
    "            mean_vals.loc[l,c] = np.mean(reduced_data.loc[loc,c])\n",
    "\n",
    "    mean_vals = mean_vals.fillna(0)\n",
    "\n",
    "\n",
    "    Agg_c12 = cluster.AgglomerativeClustering(n_clusters = 12, compute_full_tree=True,\n",
    "                                          connectivity = connectivity, affinity='euclidean', linkage='ward',)  \n",
    "    AggLabels12d = Agg_c12.fit_predict(mean_vals.values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    c = np.zeros((len(island_classes),), dtype = 'int')\n",
    "    \n",
    "    for n,i in enumerate(unique_classes):\n",
    "        \n",
    "        loc = np.where(all_classes == i)[0]\n",
    "        c[loc] = AggLabels12d[n]\n",
    "        \n",
    "    \n",
    "    labels_dropped[dropped_col] = c\n",
    "\n",
    "    \n",
    "    \n",
    "dropped_cols = ['channels','channels_MaxW', 'channels_MinW', 'channels_AvgW']\n",
    "    \n",
    "columns_s = [['AspectR','DryShapeF','FractalD','NumOutflow','Solidity','Convexity'],\n",
    "             ['Max_Width','AspectR','DryShapeF','FractalD','NumOutflow','Solidity','Convexity'],\n",
    "             ['Min_Width','AspectR','DryShapeF','FractalD','NumOutflow','Solidity','Convexity'],\n",
    "             ['Avg_Width','AspectR','DryShapeF','FractalD','NumOutflow','Solidity','Convexity']]\n",
    "\n",
    "for ii in range(4):\n",
    "\n",
    "\n",
    "    dropped_col = dropped_cols[ii]\n",
    "\n",
    "    columns = columns_s[ii]\n",
    "\n",
    "    print dropped_col\n",
    "\n",
    "    n, m = 20,20\n",
    "\n",
    "    directory = dropped_col + '_' + str(n) + 'x' + str(m)\n",
    "    print directory\n",
    "\n",
    "\n",
    "\n",
    "    n = 4\n",
    "    while n < 7:\n",
    "\n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(scaled_parameters[columns] + noise)\n",
    "\n",
    "        # TODO: Transform the good data using the PCA fit above\n",
    "        reduced_data = pca.transform(scaled_parameters[columns] + noise)\n",
    "\n",
    "\n",
    "        # Create a DataFrame for the reduced data\n",
    "        reduced_data = pd.DataFrame(reduced_data)\n",
    "        reduced_data.columns = ['PCA' + str(k) for k in reduced_data.columns.values if type(k) <> 'str']\n",
    "\n",
    "        # pd.to_pickle(reduced_data, '_PCA/pca__reduced_data.p')\n",
    "\n",
    "        pca_results = pca_results_funct(scaled_parameters[columns] + noise, pca)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        print 'Explained variance:', sum(explained_variance)*100, '%'\n",
    "\n",
    "        if sum(explained_variance)*100 < 95:\n",
    "            n += 1\n",
    "        else:\n",
    "            n = 10\n",
    "\n",
    "    nRows, nCols = 20,20\n",
    "\n",
    "    island_classes, class_num = load_shapefile('_clusterpy/classes_' + directory + '.shp', parameters=['geoSom_poly'])\n",
    "    neurons, count = load_shapefile('_PCA/' + directory + '.shp', parameters=['ID','iter999'])\n",
    "\n",
    "\n",
    "\n",
    "    count_gt_zero = [n for n,l in enumerate(count['iter999']) if l>0]\n",
    "\n",
    "    count = np.array(count['iter999']).astype('int').reshape((nRows,nCols))\n",
    "    label_count = [i for i in count.flatten() if i > 0]\n",
    "\n",
    "    unique_classes = np.unique(class_num['geoSom_poly'])\n",
    "    all_classes = np.array(class_num['geoSom_poly'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    neurons2 = MultiPolygon([i for n,i in enumerate(neurons) if count.flatten()[n] > 0.0])\n",
    "    W = ps.weights.Queen.from_iterable(neurons2)\n",
    "    connectivity = W.full()[0]\n",
    "\n",
    "\n",
    "\n",
    "    new_classes = np.zeros((len(all_classes),), dtype = 'int')\n",
    "\n",
    "    for n,i in enumerate(unique_classes):\n",
    "\n",
    "        loc = all_classes == i\n",
    "        new_classes[loc] = count_gt_zero[n]\n",
    "\n",
    "    all_classes = new_classes.copy()\n",
    "\n",
    "    mean_vals = pd.DataFrame(index = np.unique(all_classes))\n",
    "\n",
    "    for c in reduced_data.columns:\n",
    "\n",
    "        for n,l in enumerate(np.unique(all_classes)):\n",
    "\n",
    "            loc = list(np.where(all_classes == l)[0])\n",
    "\n",
    "            mean_vals.loc[l,c] = np.mean(reduced_data.loc[loc,c])\n",
    "\n",
    "    mean_vals = mean_vals.fillna(0)\n",
    "\n",
    "\n",
    "    Agg_c12 = cluster.AgglomerativeClustering(n_clusters = 12, compute_full_tree=True,\n",
    "                                          connectivity = connectivity, affinity='euclidean', linkage='ward',)  \n",
    "    AggLabels12d = Agg_c12.fit_predict(mean_vals.values)\n",
    "\n",
    "\n",
    "\n",
    "    c = np.zeros((len(island_classes),), dtype = 'int')\n",
    "\n",
    "    for n,i in enumerate(unique_classes):\n",
    "\n",
    "        loc = np.where(all_classes == i)[0]\n",
    "        c[loc] = AggLabels12d[n]\n",
    "\n",
    "\n",
    "    labels_dropped[dropped_col] = c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
